{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7d169d",
   "metadata": {},
   "source": [
    "\n",
    "# Task‑01 — Linear Regression on House Prices (Kaggle)\n",
    "\n",
    "**Goal:** Build a simple **Linear Regression** model to predict `SalePrice` (house price) using **only**:\n",
    "- `GrLivArea` — above-ground living area (square feet)\n",
    "- `BedroomAbvGr` — number of bedrooms above ground\n",
    "- `FullBath` — number of full bathrooms\n",
    "\n",
    "We’ll proceed step-by-step and explain *what* we do and *why*. This notebook assumes you have these files in the **same folder**:\n",
    "- `train.excel` (or `train.xlsx` / `train.csv`)\n",
    "- `test.excel` (or `test.xlsx` / `test.csv`)\n",
    "- `sample_submission.excel` (optional, just for reference)\n",
    "- `data_description.txt` (optional, for field descriptions)\n",
    "\n",
    "> **Note:** The official Kaggle competition uses CSV files; if your files have `.excel`, rename to `.xlsx` if needed. The code below auto-detects the extension and uses the appropriate loader.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d95e4f",
   "metadata": {},
   "source": [
    "\n",
    "## Glossary (plain‑English definitions)\n",
    "\n",
    "- **Feature (predictor, input variable):** A measurable property used to make predictions. Here: `GrLivArea`, `BedroomAbvGr`, `FullBath`.\n",
    "- **Target (label, output variable):** The value we want to predict. Here: `SalePrice`.\n",
    "- **Model:** A mathematical function that maps features to a prediction. **Linear regression** models a straight-line relationship.\n",
    "- **Training:** Feeding data to a model so it can learn patterns that map inputs to outputs.\n",
    "- **Validation / Hold‑out set:** A portion of data not used during training, to fairly evaluate how well the model generalizes.\n",
    "- **Overfitting:** When a model memorizes the training data but performs poorly on new data.\n",
    "- **Underfitting:** When a model is too simple and fails to capture important patterns.\n",
    "- **Coefficient (weight):** The number the model learns for each feature; it says how much the prediction changes when that feature increases by 1 unit (holding others fixed).\n",
    "- **Intercept (bias):** The base value of the prediction when all features are zero.\n",
    "- **Residual:** The difference between the true target and the model’s prediction (`y_true − y_pred`).\n",
    "- **RMSE (Root Mean Squared Error):** A common error metric; lower is better. Roughly, the typical size of prediction errors.\n",
    "- **R² (Coefficient of Determination):** Measures how much variance in the target is explained by the model (1.0 is perfect; can be negative if very poor).\n",
    "- **Imputation:** Filling in missing values so algorithms can run.\n",
    "- **Baseline:** A simple method (like predicting the mean) to sanity-check whether our model actually learns anything.\n",
    "- **Assumptions (for linear regression):** Roughly linear relationships, errors with constant spread (homoscedasticity), limited multicollinearity among features, and independent errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f0d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally for the first time, uncomment to install dependencies:\n",
    "# !pip install -q pandas scikit-learn matplotlib numpy openpyxl\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smart_read(path: str):\n",
    "    \"\"\"\n",
    "    Read a file that may be .csv, .xlsx/.xls, or a non-standard '.excel' extension.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    suffix = p.suffix.lower()\n",
    "    if suffix == '.csv':\n",
    "        return pd.read_csv(p)\n",
    "    elif suffix in ('.xlsx', '.xls', '.xlsm'):\n",
    "        return pd.read_excel(p)\n",
    "    elif suffix == '.excel':\n",
    "        # Treat like Excel\n",
    "        return pd.read_excel(p)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension for {path}. Please use .csv or .xlsx/.xls.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9940b647",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1 — Load the dataset\n",
    "\n",
    "We’ll load the **training** and **test** datasets. The training data has both **features** and the **target** (`SalePrice`). The test data has features only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c75566",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_PATH = 'train.excel'  # change if your file is named differently (e.g., 'train.csv' or 'train.xlsx')\n",
    "TEST_PATH  = 'test.excel'   # change if needed\n",
    "SAMPLE_SUB_PATH = 'sample_submission.excel'  # optional\n",
    "\n",
    "train_df = smart_read(TRAIN_PATH)\n",
    "test_df = smart_read(TEST_PATH)\n",
    "\n",
    "# Peek at the columns and first few rows\n",
    "print('Train shape:', train_df.shape)\n",
    "print('Test shape :', test_df.shape)\n",
    "display(train_df.head())\n",
    "display(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac597bd3",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2 — Select features and the target\n",
    "\n",
    "For this baseline, we’ll use **exactly three features**:\n",
    "- `GrLivArea` (square footage above ground)\n",
    "- `BedroomAbvGr` (number of bedrooms above ground)\n",
    "- `FullBath` (number of full bathrooms)\n",
    "\n",
    "The **target** is `SalePrice`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEATURES = ['GrLivArea', 'BedroomAbvGr', 'FullBath']\n",
    "TARGET = 'SalePrice'\n",
    "\n",
    "missing_cols = [c for c in FEATURES + [TARGET] if c not in train_df.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"Missing columns in training data: {missing_cols}. Check your file names or dataset.\")\n",
    "\n",
    "X = train_df[FEATURES].copy()\n",
    "y = train_df[TARGET].copy()\n",
    "\n",
    "# Basic sanity checks\n",
    "print('Missing in X:\\n', X.isna().sum())\n",
    "print('Missing in y:', y.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e154bef",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3 — Quick sanity checks (EDA)\n",
    "\n",
    "We’ll look at simple scatter plots to see whether relationships look *roughly* linear and at least sensible. In practice, more features (like neighborhoods, quality scores) matter a lot in this competition, but we intentionally keep it simple for learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scatter plots: each feature vs SalePrice\n",
    "for col in FEATURES:\n",
    "    plt.figure()\n",
    "    plt.scatter(train_df[col], y, alpha=0.5)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.title(f'{col} vs SalePrice')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e038fb",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4 — Train/validation split\n",
    "\n",
    "We hold out a **validation** set to estimate generalization. This helps detect **overfitting** and **underfitting**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebe675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train.shape, X_val.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384bd094",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5 — Handle missing values (imputation)\n",
    "\n",
    "Linear models can’t handle `NaN`s. We’ll impute any missing values with the **median** of that column (robust to outliers). For these specific columns, missingness is rare, but we do this defensively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.fillna(X_train.median(numeric_only=True))\n",
    "X_val   = X_val.fillna(X_train.median(numeric_only=True))  # use train stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737f73d",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6 — Build a **baseline**\n",
    "\n",
    "Before modeling, compare to a trivial baseline: always predict the **mean** of `SalePrice` from the training set. If our linear model can’t beat this, something is wrong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_pred = np.full_like(y_val, fill_value=y_train.mean(), dtype=np.float64)\n",
    "baseline_rmse = mean_squared_error(y_val, baseline_pred, squared=False)\n",
    "baseline_r2   = r2_score(y_val, baseline_pred)\n",
    "print(f'Baseline RMSE: {baseline_rmse:,.2f}')\n",
    "print(f'Baseline R^2 : {baseline_r2:,.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b666ca",
   "metadata": {},
   "source": [
    "\n",
    "## Step 7 — Train a **Linear Regression** model\n",
    "\n",
    "**Why linear regression?** It’s simple, fast, and interpretable:\n",
    "- Each feature gets a **coefficient** representing how much price changes per unit of that feature (holding others constant).\n",
    "- We can inspect these coefficients to understand the model’s reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c27dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "print('Intercept (bias):', linreg.intercept_)\n",
    "coef_table = pd.DataFrame({'feature': FEATURES, 'coefficient': linreg.coef_})\n",
    "display(coef_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c268a8",
   "metadata": {},
   "source": [
    "\n",
    "## Step 8 — Evaluate on the validation set\n",
    "\n",
    "We’ll compute **RMSE** and **R²**. Lower RMSE is better; higher R² is better (max 1.0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_val_pred = linreg.predict(X_val)\n",
    "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "r2   = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f'Validation RMSE: {rmse:,.2f}')\n",
    "print(f'Validation R^2 : {r2:,.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd540162",
   "metadata": {},
   "source": [
    "\n",
    "## Step 9 — Residual diagnostics (basic)\n",
    "\n",
    "We want residuals (errors) to look like random noise with fairly constant spread. Systematic patterns suggest nonlinearity or missing features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d3d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "residuals = y_val - y_val_pred\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_val_pred, residuals, alpha=0.5)\n",
    "plt.axhline(0, linestyle='--')\n",
    "plt.xlabel('Predicted SalePrice')\n",
    "plt.ylabel('Residual (y_true - y_pred)')\n",
    "plt.title('Residuals vs Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b77e7",
   "metadata": {},
   "source": [
    "\n",
    "## Step 10 — Retrain on **all** training data and predict the **test** set\n",
    "\n",
    "Finally, train on the full dataset and produce predictions for `test`. We’ll impute missing values using medians from the full training set, and save a Kaggle‑ready `submission.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Refit on ALL training data\n",
    "X_full = train_df[FEATURES].copy().fillna(train_df[FEATURES].median(numeric_only=True))\n",
    "y_full = train_df[TARGET].copy()\n",
    "\n",
    "linreg_full = LinearRegression()\n",
    "linreg_full.fit(X_full, y_full)\n",
    "\n",
    "# Prepare test features\n",
    "X_test = test_df[FEATURES].copy()\n",
    "X_test = X_test.fillna(X_full.median(numeric_only=True))\n",
    "\n",
    "test_preds = linreg_full.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_df['Id'],\n",
    "    'SalePrice': test_preds\n",
    "})\n",
    "submission_path = 'submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a0ee1",
   "metadata": {},
   "source": [
    "\n",
    "## Where to go next\n",
    "\n",
    "- Try a **log transform** of `SalePrice` (use `np.log1p` for train and `np.expm1` to invert predictions) to reduce the impact of very expensive homes.\n",
    "- Add more informative features (overall quality, year built, garage, neighborhood) and try **regularized** linear models (**Ridge** / **Lasso**) to handle multicollinearity and feature selection.\n",
    "- Consider **cross-validation** (e.g., `KFold`) instead of a single train/validation split for more reliable estimates.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}